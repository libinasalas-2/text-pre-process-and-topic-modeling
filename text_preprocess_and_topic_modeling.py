# -*- coding: utf-8 -*-
"""text preprocess and topic modeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FhnJUsJ41InrHwZxKaTDN8nyCex2ePpp
"""

#package for scraping
!pip install bs4

from bs4 import BeautifulSoup as bs
#sending request to access the content
import requests
#writing it in csv format
from csv import writer 
import pandas as pd

links = ['https://www.skymetweather.com/forecast/weather/india/tamil%20nadu/coimbatore/coimbatore/extended-forecast',
        'https://www.weather-forecast.com/']
with open('schooldrinkingwater.csv','w',encoding='utf8',newline='') as f:
  thewriter=writer(f)   
  header=['websites'] 
  thewriter.writerow(header)  
  for link in links:
    response = requests.get(link)
    html = response.content
    soup = bs(html,'html.parser')
    paragraphs = soup.find_all('p')
    for p in paragraphs:
      para = p.text
      info=[para]
      thewriter.writerow(info)

df=pd.read_csv('/content/schooldrinkingwater.csv')
df

df=df.dropna(how='all')
df

#TEXT PREPROCESSING
import string
string.punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
df['puntuation free text']= df['websites'].apply(lambda x:remove_punctuation(x))
#Lowering the book name
df['lower']= df['puntuation free text'].apply(lambda x: x.lower())

#Tokenization
from nltk.tokenize import TweetTokenizer as tt
#applying function to the column
tokenizer = tt()      
df['tokenized_books'] = df['lower'].apply(lambda x: tokenizer.tokenize(x))

#Removing stop words
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopword = stopwords.words('english')
def remove_stopwords(text):
    output= [i for i in text if i not in stopword]
    return output
df['no_stopwords']= df['tokenized_books'].apply(lambda x:remove_stopwords(x))

#Stemming 
from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
#Defining a function for stemming
def stemming(text):
    stem_tweet = [porter_stemmer.stem(word) for word in text]
    return stem_tweet
df['stemmed_books']=df['no_stopwords'].apply(lambda x: stemming(x))

#Lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
wordnet_lemmatizer = WordNetLemmatizer()
def lemmatizer(stemmed_books):
    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in stemmed_books]
    return lemm_text
df['lemmatized_books']=df['stemmed_books'].apply(lambda x:lemmatizer(x))
display(df.head())
df.to_json("ins_company.json")

#TOPIC MODELLING
!pip install pyLDAvis

import numpy as np
import json
import glob

#Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

#spacy
import spacy
import nltk
from nltk.corpus import stopwords

#Vis
import pyLDAvis
import pyLDAvis.gensim

import warnings 
warnings.filterwarnings("ignore",category=DeprecationWarning)

def load_data(file):
  with open(file,"r",encoding="utf-8") as f:
    data=json.load(f)
  return(data)

def write_data(file,data):
  with open(file,"w",encoding="utf-8") as f:
    json.dump(data,f,indent=4)

nltk.download("stopwords")
print(stopwords)

data=load_data("/content/ins_company.json")["websites"]
data=data["5"]
data=data.split(".")
data

def lemmatization(texts,allowed_postages=["NOUN","ADJ","VERB","ADV"]):
  nlp=spacy.load("en_core_web_sm",disable=["senter","ner","parser"])
  texts_out=[]
  for text in texts:
    doc=nlp(text)
    new_text=[]
    for token in doc:
      if token.pos_ in allowed_postages:
        new_text.append(token.lemma_)
    final=" ".join(new_text)
    texts_out.append(final)
  return (texts_out)

lemmatized_texts=lemmatization(data)
lemmatized_texts

def gen_words(texts):
  final=[]
  for text in texts:
    new=gensim.utils.simple_preprocess(text,deacc=True)
    final.append(new)
  return(final)

data_words=gen_words(lemmatized_texts)
print(data_words)

id2word=corpora.Dictionary(data_words)
corpus=[]
for text in data_words:
  new = id2word.doc2bow(text)
  corpus.append(new)

print(corpus)

word = id2word
print(word)

lda_model= gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=30,random_state=100,update_every=1,
                                           chunksize=100,passes=10,alpha="auto")

pyLDAvis.enable_notebook()
vis=pyLDAvis.gensim.prepare(lda_model,corpus,id2word,mds="mmds",R=30)
vis